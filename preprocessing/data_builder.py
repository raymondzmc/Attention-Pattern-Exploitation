import glob
import hashlib
import os
from os.path import join as pjoin
import torch
from transformers import BertTokenizer
from preprocessing.utils import clean, hashhex, greedy_selection
from tqdm import tqdm
import pdb


def process_cnndm(f_path, lower):
    """
    Process the tokenized CNN/DailyMail dateset with source and target tokens
    """
    src = []
    tgt = []
    flag = False
    with open(f_path, 'r') as f:
        for sent in f:
            if (lower):
                sent = sent.lower()
            tokens = sent.split()

            if len(tokens) == 0:
                continue

            if (tokens[0] == '@highlight'):
                flag = True
                tgt.append([])
                continue
            if (flag):
                tgt[-1].extend(tokens)
            else:
                src.append(tokens)

    # Remove noise tokens (ex. lrb, rrb, etc.)
    src = [clean(' '.join(sent)).split() for sent in src]
    tgt = [clean(' '.join(sent)).split() for sent in tgt]
    return src, tgt


class BertData(object):
    def __init__(self, args):
        self.args = args
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',
                                                       do_lower_case=True,
                                                       do_basic_tokenize=args.do_basic_tokenize)
        self.sep_token = '[SEP]'
        self.cls_token = '[CLS]'
        self.pad_token = '[PAD]'
        # self.tgt_bos = '[unused0]'
        # self.tgt_eos = '[unused1]'
        # self.tgt_sent_split = '[unused2]'
        self.sep_vid = self.tokenizer.vocab[self.sep_token]
        self.cls_vid = self.tokenizer.vocab[self.cls_token]
        self.pad_vid = self.tokenizer.vocab[self.pad_token]

    def preprocess(self, src, tgt, sent_labels, is_test=False):

        if ((not is_test) and len(src) == 0):
            return None

        original_src_txt = [' '.join(s) for s in src]

        # Filter source sentences to exclude those below the minimum token count
        idxs = [i for i, s in enumerate(src) if (len(s) > self.args.min_src_ntokens_per_sent)]

        # Create a binary sentence label mask
        _sent_labels = [0] * len(src)
        for l in sent_labels:
            _sent_labels[l] = 1

        # Truncate source sentence to be within the maximum token count 
        src = [src[i][:self.args.max_src_ntokens_per_sent] for i in idxs]

        # The oracle extractive summary according the binary sentence label
        sent_labels = [_sent_labels[i] for i in idxs]

        # Truncate the source and labels to be with the maximum sentence count 
        src = src[:self.args.max_src_nsents]
        sent_labels = sent_labels[:self.args.max_src_nsents]

        # If the number of sentences less than the minimum sentence count
        if ((not is_test) and len(src) < self.args.min_src_nsents):
            return None

        # Separate the source sentences with [SEP] and [CLS] tokens
        src_txt = [' '.join(sent) for sent in src]
        text = ' {} {} '.format(self.sep_token, self.cls_token).join(src_txt)

        # Sub-word tokens and IDs created by BERT tokenizer
        src_subtokens = self.tokenizer.tokenize(text)
        src_subtokens = [self.cls_token] + src_subtokens + [self.sep_token]
        src_subtoken_idxs = self.tokenizer.convert_tokens_to_ids(src_subtokens)

        # Use separator tokens to determine the length of each sentence
        _segs = [-1] + [i for (i, t) in enumerate(src_subtoken_idxs) if t == self.sep_vid]
        segs = [_segs[i] - _segs[i - 1] for i in range(1, len(_segs))]

        # Create binary masks as segment (sentence) embeddings 
        segments_ids = []
        for i, s in enumerate(segs):
            if (i % 2 == 0):
                segments_ids += s * [0]
            else:
                segments_ids += s * [1]

        cls_ids = [i for i, t in enumerate(src_subtoken_idxs) if t == self.cls_vid]
        sent_labels = sent_labels[:len(cls_ids)]

        # Create token list and IDs for target string (for abstractive summarization)
        tgt_subtokens_str = '[unused0] ' + ' [unused2] '.join(
            [' '.join(self.tokenizer.tokenize(' '.join(tt))) for tt in tgt]) + ' [unused1]'
        tgt_subtoken = tgt_subtokens_str.split()[:self.args.max_tgt_ntokens]
        if ((not is_test) and len(tgt_subtoken) < self.args.min_tgt_ntokens):
            return None
        tgt_subtoken_idxs = self.tokenizer.convert_tokens_to_ids(tgt_subtoken)

        tgt_txt = '<q>'.join([' '.join(tt) for tt in tgt])
        src_txt = [original_src_txt[i] for i in idxs]

        return src_subtoken_idxs, sent_labels, tgt_subtoken_idxs, segments_ids, cls_ids, src_txt, tgt_txt


def format_to_bert(args):

    corpus_types = ['val', 'test', 'train']
    if args.dataset in corpus_types:
        corpus_types = [args.dataset]

    # Create a dict containing the hashed names for each dataset
    corpus_mapping = {}
    for corpus_type in corpus_types:
        temp = []
        for line in open(pjoin(args.map_path, 'all_' + corpus_type + '.txt')):
            temp.append(hashhex(line.strip()))
        corpus_mapping[corpus_type] = {key.strip(): 1 for key in temp}

    # Create a list of dataset files according to the dict
    corpora = {corpus_type: [] for corpus_type in corpus_types}
    train_files, val_files, test_files = [], [], []
    for f in glob.glob(pjoin(args.input_path, '*.story')):
        f_name = f.split('/')[-1].split('.')[0]
        for corpus_type in corpora.keys():
            if f_name in corpus_mapping[corpus_type]:
                corpora[corpus_type].append(f)

    bert_data = BertData(args)


    # Construct BERT input for the corpora
    for corpus_type in corpora.keys():
        # dataset = []

        output_path = pjoin(args.output_path, corpus_type)
        if not os.path.exists(output_path):
            os.makedirs(output_path, exist_ok=True)

        for f_path in tqdm(corpora[corpus_type]):
            src, tgt = process_cnndm(f_path, args.lower)
            # dataset.append((src, tgt))
            sent_labels = greedy_selection(src[:args.max_src_nsents], tgt, 3)
            b_data = bert_data.preprocess(src, tgt, sent_labels, is_test=(corpus_type=='test'))

            if (b_data is None):
                continue

            src_subtoken_idxs, sent_labels, tgt_subtoken_idxs, segments_ids, cls_ids, src_txt, tgt_txt = b_data
            b_data_dict = {"src": src_subtoken_idxs, "tgt": tgt_subtoken_idxs,
                           "src_sent_labels": sent_labels, "segs": segments_ids, 'clss': cls_ids,
                           'src_txt': src_txt, "tgt_txt": tgt_txt}

            save_path = pjoin(output_path, f"{os.path.basename(f_path).split('.')[0]}.pt")
            torch.save(b_data_dict, save_path)