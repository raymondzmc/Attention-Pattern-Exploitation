import os
import gc
import argparse
import datetime
import logging
import random
import torch
import pathlib
import itertools
import collections

from models.summarization import Summarizer
from models.optimizers import build_optim
from models.utils import robust_load_state_dict
from data.dataloader import get_dataloader
from utils.logger import init_logger
from utils.trainer import Trainer
from utils.auxiliaries import save_cfg, load_cfg
from os.path import join as pjoin

import pdb


def train(args):
    torch.backends.cudnn.deterministic = True

    args.dataset_type = 'train'
    train_loader = get_dataloader(args, 'train')
    val_loader = get_dataloader(args, 'valid')
    checkpoint_dir = os.path.join(args.result_dir, 'checkpoints')

    model = get_model(args)
    model.train()

    optim = build_optim(args, model)

    if args.resume_checkpoint != None:
        try:
            model.load_state_dict(torch.load(args.resume_checkpoint, map_location=args.device)['model'])
        except RuntimeError:
            state_dict = torch.load(args.resume_checkpoint, map_location=args.device)['model']
            new_state_dict = collections.OrderedDict()
            for key, item in state_dict.items():
                module_names = key.split('.')
                if module_names[0] == 'bert':
                    module_names[0] = 'encoder' 
                new_key = '.'.join(module_names)
                new_state_dict[new_key] = item
            model.load_state_dict(new_state_dict)


    trainer = Trainer(args,
                      model,
                      train_loader,
                      val_loader,
                      optim)

    if args.load_ckpt:
        ckpt_dir = pjoin(args.result_dir, 'checkpoints')
        ckpts = os.listdir(ckpt_dir)
        last_ckpt = sorted(ckpts, key=lambda x: int(x.split('.')[0].split('_')[1]))[-1]
        trainer.load_checkpoint(torch.load(pjoin(ckpt_dir, last_ckpt), map_location=args.device))


    trainer.train()


def eval(args, mode='test'):
    logger = logging.getLogger()

    model = get_model(args)
    model.eval()

    dataloader = get_dataloader(args, mode)
    ckpt_dir = pjoin(args.result_dir, 'checkpoints')
    ckpts = os.listdir(ckpt_dir)

    start, end, every = None, None, None

    # # Use the same metric as validation for selecting the top checkpoints
    if args.valid_metric == 'rouge':
        metric_idx = -1
        reverse = True 
    elif args.valid_metric == 'loss':
        metric_idx = 3
        reverse = False
    elif args.valid_metric == 'step':
        metric_idx = 1
        reverse = False

    ckpts = sorted(ckpts, key=lambda x: int(x.split('.')[0].split('_')[metric_idx]), reverse=reverse)[:args.top_n_ckpts]

    if args.valid_metric == 'step':
        start, end, every = 8000, 24000, 2
        ckpts = [x for x in ckpts if int(x.split('.')[0].split('_')[metric_idx]) > start and int(x.split('.')[0].split('_')[metric_idx]) <= end]
        idx = [i for i in range(1, len(ckpts), 2)]
        ckpts = [ckpts[i] for i in idx]

    for ckpt in ckpts:
        info = ckpt.split('_')
        if len(info) == 4:
            step, loss = int(info[1]), info[3]
            logger.info(f'Results for Step {step}; Validation Loss: {loss};')
        elif len(info) == 6:
            step, loss, rouge = int(info[1]), info[3], info[5]
            logger.info(f'Results for Step {step}; Validation Loss: {loss}; Average ROUGE: {rouge}')

        ckpt_path = pjoin(ckpt_dir, ckpt)
        robust_load_state_dict(model, torch.load(ckpt_path, map_location=args.device)['model'])


        # Only compute ROUGE for the test set 
        report_rouge = (mode == 'test') and args.report_rouge
        report_f1 = (args.report_f1)
        evaluate_model(model, args, dataloader=dataloader, rouge=report_rouge, f1=report_f1, step=step, rouge_dir=args.rouge_dir)


def get_model(args):
    model = Summarizer(args)
    model.to(args.device)
    return model


def main(args):
    asctime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    # Create directory for saving results
    if args.result_dir == None:
        if args.mode == 'train':
            args.result_dir = pjoin(CWD, 'results', f'{args.encoder_type}-{args.dataset}-{asctime}')
        else:
            print('Must specify a "result_dir" for testing!')
            exit(-1)

    os.makedirs(args.result_dir, exist_ok=True)

    config_path = pjoin(args.result_dir, 'arguments.cfg')

    if args.load_ckpt and os.path.isfile(config_path):
        _args = load_cfg(config_path)
        _args.__dict__.update(vars(args))
        args = _args
    elif args.mode == 'train':
        save_cfg(args, config_path)


    # Set seeds
    torch.manual_seed(args.seed)
    random.seed(args.seed)

    # Set device (assume one GPU)
    if args.device.isdigit() and torch.cuda.is_available():
        if int(args.device) < torch.cuda.device_count():
            args.device = torch.device(f'cuda:{args.device}')
            torch.cuda.manual_seed(args.seed)
            torch.cuda.set_device(args.device)
        else:
            raise Exception(f'Invalid device index for \"cuda:{args.device}\"')
    else:
        print(f"CUDA available: {torch.cuda.is_available()}")
        args.device = torch.device('cpu')

    data_path = pjoin(args.data_dir, args.dataset)
    if not os.path.exists(data_path):
        raise Exception(f"Cannot find dataset at \"{data_path}\"")


    # Initialize logger 
    if args.log_file == None:
        args.log_file = f'{asctime}.log'
    log_file_path = pjoin(args.result_dir, args.log_file)
    logger = init_logger(log_file_path)
    logger.info(args)


    hyp_path = args.result_dir+'/bert_ext.candidate/'
    ref_path= args.result_dir+'/bert_ext.gold/'
    if not os.path.exists(hyp_path):
        os.makedirs(hyp_path)
    if not os.path.exists(ref_path):
        os.makedirs(ref_path)


    if args.mode == 'train':
        train(args)
    elif args.mode == 'test':
        eval(args, 'test')
    elif args.mode == 'valid':
        eval(args, 'valid')


if __name__ == '__main__':
    CWD = '.'

    parser = argparse.ArgumentParser()
    parser.add_argument('-mode', default='train', type=str, choices=['train', 'test', 'valid'])
    parser.add_argument('-device', default='0', type=str) # Used string for cpu

    # For PLM injection
    parser.add_argument('-use_pals', action='store_true', default=False)
    parser.add_argument('-use_pal_patterns', action='store_true', default=False)
    

    # Path arguments
    parser.add_argument('-result_dir', default=None, type=str)
    parser.add_argument('-data_dir', default=pjoin(CWD, 'data'))
    parser.add_argument('-log_file', default=None, type=str)
    parser.add_argument('-rouge_dir', default=pjoin(CWD, 'rouge_wrapper'), type=str)


    # Model specific arguments
    parser.add_argument('-encoder_type', default='bert', type=str, choices=['bert', 'roberta', 'transformer'], help='Type of Transformer encoder')
    parser.add_argument('-config_file', default=None, type=str, help='Configuration for vanilla Transformer') 
    parser.add_argument('-tokenizer_type', default='bert', type=str, choices=['bert', 'roberta'])
    parser.add_argument('-max_input_len', default=512, type=int)
    # Confirm these arguments are not needed

    # Arguments for masked/synthesized Summarizar
    parser.add_argument('-pretrained_embedding', default=None, type=str, choices=[None, 'bert', 'roberta'])
    parser.add_argument("-match_token_attn", default=None, type=str, choices=[None, 'mask', 'synthesize'])
    parser.add_argument("-inter_sent_attn", default=None, type=str, choices=[None, 'mask', 'synthesize'])
    parser.add_argument("-positional_attn", default=None, type=str, choices=[None, 'mask', 'synthesize'])
    parser.add_argument("-positional_range", default=1, type=int, help='Number of preceeding/succeeding tokens for positional attention')

    # Data Arguments
    parser.add_argument('-dataset', default=None, type=str, choices=dataset_choices, required=True, help="The dataset to run.")
    parser.add_argument('-batch_size', default=4, type=int)
    parser.add_argument('-num_workers', default=0, type=int, help='Number of workers in DataLoader, default to zero for Iterable Dataset')
    parser.add_argument('-eval_batch_size', default=24, type=int)

    # Training arguments
    parser.add_argument('-resume_checkpoint', default=None, type=str)
    parser.add_argument('-accum_batch_size', default=36, type=int) # True batch size, should be a multiple of batch_size
    parser.add_argument('-save_checkpoint_steps', default=500, type=int)
    parser.add_argument('-avoid_saving_steps', default=0, type=int)
    parser.add_argument('-report_steps', default=100, type=int)
    parser.add_argument('-param_init', default=0, type=float)
    parser.add_argument('-param_init_glorot', default=True, type=bool)
    parser.add_argument('-dropout', default=0.1, type=float)
    parser.add_argument('-top_n_ckpts', default=10, type=int, help='Number of checkpoints to keep')
    parser.add_argument('-train_steps', default=50000, type=int)
    parser.add_argument('-valid_metric', default='loss', type=str, choices=['step', 'loss', 'rouge'], help='Validation metric used for selecting checkpoints')

    # Optimizer arguments
    parser.add_argument('-optim', default='adam', type=str, choices=['adam'])
    parser.add_argument('-max_grad_norm', default=0, type=float)
    parser.add_argument('-lr', default=2e-3, type=float)
    parser.add_argument('-warmup_steps', default=10000, type=int)
    parser.add_argument('-decay_method', default='noam', type=str, choices=['noam'])
    parser.add_argument('-beta1', default= 0.9, type=float)
    parser.add_argument('-beta2', default=0.999, type=float)

    # Evaluation arguments
    parser.add_argument('-top_n', default=3, type=int, help='Number of extractive segments to keep for summarization')
    parser.add_argument('-block_trigram', action='store_true', default=False)
    parser.add_argument('-report_f1', action='store_true')
    parser.add_argument('-report_rouge', action='store_true')
    parser.add_argument('-rouge_metric', default='f1', type=str, choices=['f1', 'recall'])
    parser.add_argument('-limit_length', action='store_true', default=False, help='Whether to truncate the predicted summaries to the lengths of the gold summaries')
    parser.add_argument('-save_score', action='store_true', default=True)

    parser.add_argument('-debug', action='store_true', default=False, help='Whether to print out error messages')
    parser.add_argument('-load_ckpt', action='store_true', default=False, help='Whether to load a saved checkpoint in "result_dir"')

    parser.add_argument('-seed', default=10, type=int)
    parser.add_argument('-heads_per_pattern', default=1, type=int)
    args = parser.parse_args()

    main(args)


    