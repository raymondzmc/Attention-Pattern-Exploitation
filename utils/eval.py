import os
import shutil
import torch
import logging
import numpy as np
import pandas as pd
from tqdm import tqdm
from utils.pyrouge import Rouge155
from os.path import join as pjoin
from utils.eval_utils import get_rouge, ngram_overlap
from scipy.spatial.distance import jensenshannon
import pdb


def evaluate_model(model, args, dataloader=None, rouge=True, f1=False, step=None, rouge_dir=None):
    logger = logging.getLogger()
    candidate_path = pjoin(args.result_dir, 'bert_ext.candidate')
    gold_path = pjoin(args.result_dir, 'bert_ext.gold')

    loss_function = torch.nn.BCELoss(reduction='none')
    total_loss = 0.
    all_hyp_path = []
    all_ref_path = []
    all_ids = []
    all_selections = []
    all_oracle = []

    tp, fp, fn = 0, 0, 0

    dynamic_attn_mask = (args.summarizer_type == 'oursum')
    with torch.no_grad():
        for i, batch in enumerate(tqdm(dataloader)):
            src_subtoken_idxs = batch.src.to(args.device)
            segments_ids = batch.segs.to(args.device)
            cls_idx = batch.clss.to(args.device)
            attention_mask = batch.mask.to(args.device)
            cls_mask = batch.mask_cls.to(args.device)
            labels = batch.labels.to(args.device)
            cls_idx[cls_idx >= args.max_input_len] = 0
            cls_mask[cls_idx >= args.max_input_len] = 0
            src_txt = batch.src_txt
            tgt_txt = batch.tgt_txt

            segments_ids = batch.segs.to(args.device)
            sent_mask = batch.sent_mask.to(args.device)
            sent_span = batch.sent_span
            tree_attention = None
            # if use_tree:
            #     tree_attention=batch.attn_map.to(args.device)

            sent_scores = model(src_subtoken_idxs, 
                                segments_ids, 
                                attention_mask, 
                                cls_idx, 
                                cls_mask,
                                dynamic_attn_mask=dynamic_attn_mask,
                                # tree_attention=tree_attention,
                                sent_span=sent_span,
                                sent_mask=sent_mask)
            loss = loss_function(sent_scores, labels)
            loss = (loss * cls_mask).sum().item()
            total_loss += loss

            if f1:
                # targets = labels
                # pred = sent_scores.round()
                # tp += torch.logical_and(targets == 1, pred == 1).sum().item()
                # fp += torch.logical_and(targets == 0, pred == 1).sum().item()
                # fn += torch.logical_and(targets == 1, pred == 0).sum().item()

                scores = sent_scores + cls_mask
                scores = scores.cpu().data.numpy()
                
                # Sort the sentences by their extractive score
                sorted_scores = np.argsort(-scores, 1)

                for batch_idx, _sorted_scores in enumerate(sorted_scores):
                    targets = labels[batch_idx]
                    pred = torch.zeros_like(targets)
                    pred[_sorted_scores[:args.top_n]] = 1
                    tp += torch.logical_and(targets == 1, pred == 1).sum().item()
                    fp += torch.logical_and(targets == 0, pred == 1).sum().item()
                    fn += torch.logical_and(targets == 1, pred == 0).sum().item()

            # Compute ROUGE score for the model predictions
            if rouge:

                # Quick and dirty way to ensure non-masked cls (sentences) are not scored
                sent_scores = sent_scores + cls_mask
                sent_scores = sent_scores.cpu().data.numpy()
                
                # Sort the sentences by their extractive score
                sorted_sent_scores = np.argsort(-sent_scores, 1)

                for batch_idx, _sorted_sent_scores in enumerate(sorted_sent_scores):
                    fid = batch.ids[batch_idx]
                    oracle_ids = torch.nonzero(labels[batch_idx], as_tuple=False).squeeze(1).tolist()
                    selected_ids = []
                    pred = []

                    src_text_len = len(src_txt[batch_idx])

                    # Empty source text
                    if (src_text_len == 0):
                        continue

                    # Form prediction text from the sorted sentence scores 
                    for sent_idx in _sorted_sent_scores:

                        # Sentence scores outside the range of the source length (shouldn't happen anyways)
                        if sent_idx >= src_text_len:
                            continue

                        candidate = src_txt[batch_idx][sent_idx].strip()

                        if (not args.block_trigram) or (not ngram_overlap(candidate, pred)):
                            selected_ids.append(sent_idx)
                            pred.append(candidate)

                        if len(pred) == args.top_n:
                            break

                    hyp_path = pjoin(candidate_path,'%s.txt'%(fid))
                    with open(hyp_path,'w') as of:
                        order_summ = [y for _,y in sorted(zip(selected_ids, pred), reverse=False)]
                        of.write('\n'.join(pred))


                    ref_path=pjoin(gold_path,'%s.txt'%(fid))
                    with open(ref_path,'w') as of:
                        of.write(tgt_txt[batch_idx])
                    
                    all_ref_path.append(ref_path)
                    all_hyp_path.append(hyp_path)
                    all_ids.append(fid)
                    all_selections.append(selected_ids)
                    all_oracle.append(oracle_ids)
            
    if f1:
        try:
            precision = round(tp / (tp + fp), 4)
        except:
            precision = 0.

        try:
            recall = round(tp / (tp + fn), 4)
        except:
            recall = 0.

        f_score = round(tp / (tp + 0.5 * (fp + fn)), 4)
        logger.info(f"Precision: {precision}, Recall: {recall}, F1: {f_score}")

    if rouge:

        avg, c, df = get_rouge(all_hyp_path, all_ref_path, config_path=pjoin(args.result_dir, 'config'), rouge_dir=rouge_dir)
        avg_rouge = (avg['rouge-1-f'] + avg['rouge-2-f'] + avg['rouge-L-f']) / 3

        logger.info("Rouge-1 r score: %f, Rouge-1 p score: %f, Rouge-1 f-score: %f, 95-conf(%f-%f)"%(\
                avg['rouge-1-r'],avg['rouge-1-p'],avg['rouge-1-f'],c[0]['lower_conf_f'],c[0]['upper_conf_f']))
        logger.info("Rouge-2 r score:%f, Rouge-1 p score: %f, Rouge-2 f-score:%f, 95-conf(%f-%f)"%(\
            avg['rouge-2-r'],avg['rouge-2-p'],avg['rouge-2-f'],c[1]['lower_conf_f'],c[1]['upper_conf_f']))
        logger.info("Rouge-L r score:%f, Rouge-1 p score: %f, Rouge-L f-score:%f, 95-conf(%f-%f)"%(\
            avg['rouge-L-r'],avg['rouge-L-p'],avg['rouge-L-f'],c[2]['lower_conf_f'],c[2]['upper_conf_f']))

        if args.save_score:

            # Placeholder values for the last row
            all_selections.append([0])
            all_oracle.append([0])
            all_ids.append('avg')

            # all_attn_weight.append([0])
            df['id'] = pd.Series(all_ids, index=df.index)
            df['selections'] = pd.Series(np.array(all_selections, dtype=object), index=df.index)
            df['oracle'] = pd.Series(np.array(all_oracle, dtype=object), index=df.index)

            csv_name = 'results.csv' if step == None else f'step_{step}_results.csv'
            df.to_csv(pjoin(args.result_dir, csv_name))

        logger.info(f"Total Loss : {total_loss}, AVG Rouge: {avg_rouge}")

    else:
        logger.info(f"Total Loss : {total_loss}")

    return total_loss


def rouge_results_to_str(results_dict):
    return ">> ROUGE-F(1/2/3/l): {:.2f}/{:.2f}/{:.2f}\nROUGE-R(1/2/3/l): {:.2f}/{:.2f}/{:.2f}\n".format(
        results_dict["rouge_1_f_score"] * 100,
        results_dict["rouge_2_f_score"] * 100,
        results_dict["rouge_l_f_score"] * 100,
        results_dict["rouge_1_recall"] * 100,
        results_dict["rouge_2_recall"] * 100,
        results_dict["rouge_l_recall"] * 100
    )

def test_rouge(temp_dir, cand, ref):
    candidates = [line.strip() for line in open(cand, encoding='utf-8')]
    references = [line.strip() for line in open(ref, encoding='utf-8')]
    print(len(candidates))
    print(len(references))
    assert len(candidates) == len(references)

    cnt = len(candidates)
    if not os.path.isdir(temp_dir):
        os.makedirs(pjoin(temp_dir, "candidate"), exist_ok=True)
        os.makedirs(pjoin(temp_dir, "reference"), exist_ok=True)
    try:

        for i in range(cnt):
            if len(references[i]) < 1:
                continue
            with open(temp_dir + "/candidate/cand.{}.txt".format(i), "w",
                      encoding="utf-8") as f:
                f.write(candidates[i])
            with open(temp_dir + "/reference/ref.{}.txt".format(i), "w",
                      encoding="utf-8") as f:
                f.write(references[i])
        r = Rouge155(temp_dir=temp_dir)
        r.model_dir = temp_dir + "/reference/"
        r.system_dir = temp_dir + "/candidate/"
        r.model_filename_pattern = 'ref.#ID#.txt'
        r.system_filename_pattern = r'cand.(\d+).txt'
        rouge_results = r.convert_and_evaluate()
        print(rouge_results)
        results_dict = r.output_to_dict(rouge_results)
    finally:
        pass
        if os.path.isdir(temp_dir):
            shutil.rmtree(temp_dir)
    return results_dict
