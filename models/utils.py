import torch
import torch.nn as nn
import json
import pdb
import copy
import collections
from transformers.file_utils import (
    hf_bucket_url,
    WEIGHTS_NAME,
    cached_path,
)
import pdb

def prune_linear_layer(layer, index, dim=0):
    """
    Prune a linear layer to keep only entries in index.
    Used to remove heads.
    Args:
        layer (:obj:`torch.nn.Linear`): The layer to prune.
        index (:obj:`torch.LongTensor`): The indices to keep in the layer.
        dim (:obj:`int`, `optional`, defaults to 0): The dimension on which to keep the indices.
    Returns:
        :obj:`torch.nn.Linear`: The pruned layer as a new layer with :obj:`requires_grad=True`.
    """
    index = index.to(layer.weight.device)
    W = layer.weight.index_select(dim, index).clone().detach()
    if layer.bias is not None:
        if dim == 1:
            b = layer.bias.clone().detach()
        else:
            b = layer.bias[index].clone().detach()
    new_size = list(layer.weight.size())
    new_size[dim] = len(index)
    new_layer = nn.Linear(new_size[1], new_size[0], bias=layer.bias is not None).to(layer.weight.device)
    new_layer.weight.requires_grad = False
    new_layer.weight.copy_(W.contiguous())
    new_layer.weight.requires_grad = True
    if layer.bias is not None:
        new_layer.bias.requires_grad = False
        new_layer.bias.copy_(b.contiguous())
        new_layer.bias.requires_grad = True
    return new_layer


def init_linear_layer(layer, index, dim=0):
    """
    Prune a linear layer to keep only entries in index.
    Used to remove heads.
    Args:
        layer (:obj:`torch.nn.Linear`): The layer to prune.
        index (:obj:`torch.LongTensor`): The indices to keep in the layer.
        dim (:obj:`int`, `optional`, defaults to 0): The dimension on which to keep the indices.
    Returns:
        :obj:`torch.nn.Linear`: The pruned layer as a new layer with :obj:`requires_grad=True`.
    """
    index = index.to(layer.weight.device)

    with torch.no_grad():
        import pdb; pdb.set_trace()
        new_weights = nn.init.xavier_uniform_(layer.weight.index_select(dim, index))
        # layer.weight.index_select(dim, index) = new_weights
    layer.weight.index_select(dim, index)
    # torch.arange()
    # layer.weight[]

    W = layer.weight.index_select(dim, index).clone().detach()
    if layer.bias is not None:
        if dim == 1:
            b = layer.bias.clone().detach()
        else:
            b = layer.bias[index].clone().detach()
    new_size = list(layer.weight.size())
    new_size[dim] = len(index)
    new_layer = nn.Linear(new_size[1], new_size[0], bias=layer.bias is not None).to(layer.weight.device)
    new_layer.weight.requires_grad = False
    new_layer.weight.copy_(W.contiguous())
    new_layer.weight.requires_grad = True
    if layer.bias is not None:
        new_layer.bias.requires_grad = False
        new_layer.bias.copy_(b.contiguous())
        new_layer.bias.requires_grad = True
    return new_layer

def find_initializable_heads_and_indices(heads, n_heads, head_size, already_pruned_heads):
    """
    List, int, int, set -> Tuple[set, "torch.LongTensor"]
    """

    mask = torch.ones(n_heads, head_size)
    heads = set(heads) - already_pruned_heads  # Convert to set and remove already pruned heads
    for head in heads:
        # Compute how many pruned heads are before the head and move the index accordingly
        head = head - sum(1 if h < head else 0 for h in already_pruned_heads)
        mask[head] = 0
    init_mask = mask.view(-1).contiguous().eq(0)
    index_select = torch.arange(len(init_mask))[init_mask].long()
    return heads, index_select

def find_pruneable_heads_and_indices(heads, n_heads, head_size, already_pruned_heads):
    mask = torch.ones(n_heads, head_size)
    heads = set(heads) - already_pruned_heads  # Convert to set and remove already pruned heads
    for head in heads:
        # Compute how many pruned heads are before the head and move the index accordingly
        head = head - sum(1 if h < head else 0 for h in already_pruned_heads)
        mask[head] = 0
    mask = mask.view(-1).contiguous().eq(1)
    index: torch.LongTensor = torch.arange(len(mask))[mask].long()
    return heads, index


def prune_heads(model, heads_to_prune, in_place=True):
    if not in_place:
        model = copy.deepcopy(model)

    for layer_idx, heads in heads_to_prune.items():
        if model.bert.encoder.layer[layer_idx].attention.pruned_heads == None:
            model.bert.encoder.layer[layer_idx].attention.pruned_heads = set()
            
        attention = model.bert.encoder.layer[layer_idx].attention
        mask = torch.ones(attention.self.num_attention_heads, attention.self.attention_head_size)

        # The set of already pruned heads
        pruned_heads = attention.pruned_heads

        # Convert to set and remove already pruned heads
        heads = set(heads) - pruned_heads

        for head in heads:
            # Compute how many pruned heads are before the head and move the index accordingly
            head_idx = head - sum(1 if h < head else 0 for h in pruned_heads)
            mask[head_idx] = 0

        mask = mask.view(-1).contiguous().eq(1)
        index = torch.arange(len(mask))[mask].long()
        
        # Prune linear layers
        pruned_query = prune_linear_layer(attention.self.query, index)
        model.bert.encoder.layer[layer_idx].attention.self.query = pruned_query

        pruned_key = prune_linear_layer(attention.self.key, index)
        model.bert.encoder.layer[layer_idx].attention.self.key = pruned_key

        pruned_value = prune_linear_layer(attention.self.value, index)
        model.bert.encoder.layer[layer_idx].attention.self.value = pruned_value

        pruned_dense = prune_linear_layer(attention.output.dense, index, dim=1)
        model.bert.encoder.layer[layer_idx].attention.output.dense = pruned_dense

        # Update hyper params and store pruned heads
        model.bert.encoder.layer[layer_idx].attention.self.num_attention_heads = \
            attention.self.num_attention_heads - len(heads)

        model.bert.encoder.layer[layer_idx].attention.self.all_head_size = \
            attention.self.attention_head_size * attention.self.num_attention_heads

        model.bert.encoder.layer[layer_idx].attention.pruned_heads = \
            attention.pruned_heads.union(heads)

    return model

def load_state_dict(model, model_name, cache_dir=None):
    """
    Manually load the state dict for a pretrained model (for customized class)
    Mostly taken from transformers.PreTrainedModel.from_pretrained
    """

    archive_file = hf_bucket_url(model_name, WEIGHTS_NAME)
    resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir)
    state_dict = torch.load(resolved_archive_file, map_location=model.device)
    missing_keys = []
    unexpected_keys = []
    error_msgs = []

    # Convert old format to new format if needed from a PyTorch state_dict
    old_keys = []
    new_keys = []
    for key in state_dict.keys():
        new_key = None
        if "gamma" in key:
            new_key = key.replace("gamma", "weight")
        if "beta" in key:
            new_key = key.replace("beta", "bias")
        if new_key:
            old_keys.append(key)
            new_keys.append(new_key)
    for old_key, new_key in zip(old_keys, new_keys):
        state_dict[new_key] = state_dict.pop(old_key)


    metadata = getattr(state_dict, "_metadata", None)
    if metadata is not None:
        state_dict._metadata = metadata

    # PyTorch's `_load_from_state_dict` does not copy parameters in a module's descendants
    # so we need to apply the function recursively.
    def load(module: nn.Module, prefix=""):
        local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})
        module._load_from_state_dict(
            state_dict,
            prefix,
            local_metadata,
            True,
            missing_keys,
            unexpected_keys,
            error_msgs,
        )
        for name, child in module._modules.items():
            if child is not None:
                load(child, prefix + name + ".")

    # Make sure we are able to load base models as well as derived models (with heads)
    if hasattr(model, 'base_model_prefix'):
        start_prefix = getattr(model, 'base_model_prefix') + "."
    else: 
        start_prefix = ""

    load(model, prefix=start_prefix)

def robust_load_state_dict(model, state_dict):
    new_state_dict = collections.OrderedDict()
    for key, item in state_dict.items():
        module_names = key.split('.')
        if module_names[0] == 'bert':
            module_names[0] = 'encoder' 
        new_key = '.'.join(module_names)
        new_state_dict[new_key] = item

    model.load_state_dict(new_state_dict, strict=True)
    del new_state_dict, state_dict
    torch.cuda.empty_cache()