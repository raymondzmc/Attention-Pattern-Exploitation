import re
import torch
import torch.nn as nn

from collections import defaultdict
from transformers import AutoModel, AutoTokenizer, BertConfig, PretrainedConfig
from models.encoders import BertModel, RobertaModel
from models.utils import load_state_dict

import pdb

class CrossSegmentBert(nn.Module):
    def __init__(self, args):
        super(CrossSegmentBert, self).__init__()

        self.use_pals = args.use_pals
        self.use_pal_patterns = args.use_pal_patterns
        self.use_patterns = args.use_patterns
        self.compute_sparsity = args.compute_sparsity
        self.device = args.device
        self.heads_per_pattern = args.heads_per_pattern

        # Use local (modified) implementations of BERT and RoBERTa models
        if args.encoder in ['bert-base-uncased', 'roberta-base']:
            model_config = PretrainedConfig.from_pretrained(args.encoder)

            # Use projected attention layers 
            model_config.use_pals = self.use_pals
            model_config.use_pal_patterns = self.use_pal_patterns
            model_config.hidden_size_pals = args.hidden_size_pals
            model_config.num_attention_heads_pals = args.num_attention_heads_pals

            if args.encoder == 'bert-base-uncased':
                self.encoder = BertModel(model_config)
            elif args.encoder == 'roberta-base':
                self.encoder = RobertaModel(model_config)
            if args.from_checkpoint:
                self.encoder.load_state_dict(torch.load(args.from_checkpoint, map_location=self.encoder.device))
                self.encoder = self.encoder.distilbert
            else:
                load_state_dict(self.encoder, args.encoder)

            tokenizer = AutoTokenizer.from_pretrained(args.encoder)
            # pretrained_weights = AutoModel.from_pretrained(args.encoder).state_dict()
            # incompatible_keys = self.encoder.load_state_dict(pretrained_weights, strict=False)
            # missing_keys, unexpected_keys = incompatible_keys.missing_keys, incompatible_keys.unexpected_keys
            # assert len([key for key in missing_keys if 'pals' not in key]) == 0 and len(unexpected_keys) == 0


        elif args.encoder == 'transformer':
            # Use BERT tokenizer for identifying special tokens
            model_config = BertConfig.from_json_file(args.config_file)
            self.encoder = BertModel(model_config)
            tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

            # Use embedding weights from pretrained encoders
            if args.pretrained_embedding != None:
                self.encoder.embeddings.load_state_dict(
                    AutoModel.from_pretrained(args.pretrained_embedding).embeddings.state_dict()
                )
        else:
            self.encoder = AutoModel.from_pretrained(args.encoder, cache_dir='./.cache')
            tokenizer = AutoTokenizer.from_pretrained(args.encoder, cache_dir='./.cache')


        self.cls_id = tokenizer.cls_token_id
        self.sep_id = tokenizer.sep_token_id


        # List of tuples [(layer, head), ...] representing the attention head index for masking
        self.matching_token_heads = []
        self.positional_heads = defaultdict(list)
        self.intracontext_heads = []
        self.synthesized_heads = defaultdict(list)
        num_patterns = 4
        if self.use_patterns or self.use_pal_patterns:
            for i in range(model_config.num_hidden_layers):
                for j in range(self.heads_per_pattern):

                    if args.match_token:
                        self.matching_token_heads.append((i, num_patterns * j))

                    if args.intra_context:
                        self.intracontext_heads.append((i, num_patterns * j + 1))

                    if args.positional:
                        self.positional_heads['-1'].append((i, num_patterns * j + 2))
                        self.positional_heads['+1'].append((i, num_patterns * j + 3))
                        self.synthesized_heads[i].append(num_patterns * j + 2)
                        self.synthesized_heads[i].append(num_patterns * j + 3)



        classifier_input_size = self.encoder.config.hidden_size
        self.classifier = nn.Sequential(
            nn.Linear(classifier_input_size, args.hidden_size, bias=True),
            nn.ReLU(),
            nn.Linear(args.hidden_size, 1, bias=True),
        )
        self.criterion = nn.BCEWithLogitsLoss(reduction='mean')
        self.init_classifier()

        if args.encoder in ['roberta-base', 'distilbert-base-uncased']:
            self.use_token_type_ids = False
        else:
            self.use_token_type_ids = True
    
    def init_classifier(self):
        for p in self.classifier.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)
            else:
                nn.init.constant_(p, 0)


    def get_dynamic_mask(self, input_ids):
        cfg = self.encoder.config
        batch_size, input_len = input_ids.size()

        if self.use_pals and self.use_pal_patterns:
            mask_size = (batch_size, cfg.num_hidden_layers, cfg.num_attention_heads_pals, input_len, input_len)
        else:
            mask_size = (batch_size, cfg.num_hidden_layers, cfg.num_attention_heads, input_len, input_len)

        dynamic_attn_masks = torch.full(mask_size, True, dtype=torch.bool, device=self.encoder.device)

        if self.matching_token_heads != []:
            repeated_input_ids = input_ids.unsqueeze(1).repeat(1, input_len, 1)
            identical_mask = (repeated_input_ids.transpose(1, 2) == repeated_input_ids)
            del repeated_input_ids

            # Tokens with matching pairs does not attend to itself
            diag_mask = torch.eye(input_len).bool()
            # Special tokens will not attend to matching pairs for synthesized attention
            # if self.args.match_token_attn == 'synthesize':
            #     self.ignore_tokens = self.ignore_tokens.to(self.device)
            #     ignore_mask = torch.any(input_ids.unsqueeze(2).repeat(1, 1, len(self.ignore_tokens)) == self.ignore_tokens.repeat(batch_size, input_len, 1), dim=2)
            #     identical_mask[ignore_mask] = False
            #     del ignore_mask

            # Tokens without matching pairs will attend to all tokens
            identical_mask[identical_mask.any(dim=-1).logical_not()] = True
            identical_mask[:, diag_mask] = True

            for (layer, head) in self.matching_token_heads:
                dynamic_attn_masks[:, layer, head] = identical_mask

            del identical_mask

            with torch.cuda.device(self.device):
                torch.cuda.empty_cache()


        sep_idx = [torch.nonzero(_input_ids == self.sep_id, as_tuple=False).squeeze(1) for _input_ids in input_ids]
        if self.intracontext_heads != []:
            intra_context_mask = torch.full((batch_size, input_len, input_len), False, dtype=torch.bool, device=self.encoder.device)
            for batch_idx in range(batch_size):
                intra_context_mask[batch_idx][:sep_idx[batch_idx], :sep_idx[batch_idx]] = True
                intra_context_mask[batch_idx][sep_idx[batch_idx]:, sep_idx[batch_idx]:] = True

            # Give the [CLS] token access to both context
            intra_context_mask[:, 0] = True

            for layer, head in self.intracontext_heads:
                dynamic_attn_masks[:, layer, head] = intra_context_mask

        for key, indices in self.positional_heads.items():
            if indices != []:
                offset = int(key[1])
                positional_mask = torch.full((batch_size, input_len, input_len), False, dtype=torch.bool, device=self.encoder.device)
                
                for diagonal in range(1, offset + 1):
                    diagonal = -diagonal if key[0] == '-' else diagonal
                    offset_mask = torch.diag(torch.ones(input_len - abs(diagonal)), diagonal).bool().to(self.encoder.device)
                    positional_mask = torch.logical_or(positional_mask, offset_mask)
                
                for layer, head in indices:
                    dynamic_attn_masks[:, layer, head] = positional_mask

        if self.compute_sparsity:
            self.sparsity_ratio = (dynamic_attn_masks.int().sum().item(), dynamic_attn_masks.numel())

        # # # Allow CLS to all tokens to attend to CLS and SEP tokens
        # for batch_idx in range(batch_size):
        #     cls_sep_idx = torch.cat((cls_idx[batch_idx], sep_idx[batch_idx]))
        #     dynamic_attn_masks[batch_idx, :, :, :, cls_sep_idx] = True

        return dynamic_attn_masks

    def forward(self, input_ids, attention_mask, token_type_ids, targets, output_cls_hidden=False, output_attentions=False, output_hidden_states=False):

        if self.use_patterns or self.use_pal_patterns:

            # Forward pass using our modified implementation
            dynamic_attn_masks = self.get_dynamic_mask(input_ids)

            if self.use_token_type_ids:
                encoder_output = self.encoder(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    token_type_ids=token_type_ids,
                    output_hidden_states=output_hidden_states,
                    output_attentions=output_attentions,
                    dynamic_attn_masks=dynamic_attn_masks,
                    synthesized_heads=self.synthesized_heads,
                )
            else:
                encoder_output = self.encoder(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    output_hidden_states=output_hidden_states,
                    output_attentions=output_attentions,
                    dynamic_attn_masks=dynamic_attn_masks,
                    synthesized_heads=self.synthesized_heads,
                )
        else:

            # Forward pass using original implementation
            if self.use_token_type_ids:
                encoder_output = self.encoder(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    token_type_ids=token_type_ids,
                    output_hidden_states=output_hidden_states,
                    output_attentions=output_attentions,
                )
            else:
                encoder_output = self.encoder(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    output_hidden_states=output_hidden_states,
                    output_attentions=output_attentions,
                )

        hidden_states = encoder_output[0]
        cls_embeddings = hidden_states[:, 0, :]

        logits = self.classifier(cls_embeddings).squeeze(1)
        cls_loss = self.criterion(logits, targets)

        output = {
            'logits': logits,
            'loss': cls_loss,
        }

        if output_cls_hidden:
            output['cls_hidden'] = cls_embeddings
        if output_attentions:
            output['attentions'] = encoder_output[-1]
        if output_hidden_states:
            output['hidden_states'] = encoder_output[-2]
        if self.compute_sparsity:
            output['sparsity_ratio'] = self.sparsity_ratio

        return output