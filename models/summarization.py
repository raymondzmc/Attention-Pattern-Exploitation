import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import logging
import re
from os.path import join as pjoin

from transformers import BertConfig, PretrainedConfig, AutoTokenizer, AutoModel
from models.encoders import BertModel, RobertaModel
from utils.logger import init_logger
from models.utils import load_state_dict
from models.ext_layers import Classifier

from collections import defaultdict, OrderedDict
from typing import Any, Callable, Dict, List, Optional, Set, Tuple, Union

import pdb

encoder_name_mapping = {
    'transformer': 'bert-base-uncased',
    'bert': 'bert-base-uncased',
    'roberta': 'roberta-base',
}

class Summarizer(nn.Module):
    def __init__(self, args):
        super(Summarizer, self).__init__()
        self.args = args
        self.device = args.device
        self.logger = logging.getLogger()

        self.use_pals = getattr(args, 'use_pals', False)
        self.use_pal_patterns = getattr(args, 'use_pal_patterns', False)

        # Transformer document encoder
        if args.encoder_type in ('bert', 'roberta'):
            model_name =  encoder_name_mapping[args.encoder_type]
            model_config = PretrainedConfig.from_pretrained(model_name)

            # Use projected attention layers 
            model_config.use_pals = self.use_pals
            model_config.use_pal_patterns = self.use_pal_patterns
            model_config.hidden_size_pals = 256
            model_config.num_attention_heads_pals = 4

            self.encoder = BertModel(model_config) if args.encoder_type == 'bert' else RobertaModel(model_config)
            load_state_dict(self.encoder, model_name)

        elif args.encoder_type == 'transformer':
            # Use BERT tokenizer for identifying special tokens
            model_config = BertConfig.from_json_file(args.config_file)
            self.encoder = BertModel(model_config)

        else:
            raise Exception(f'Encoder type "{args.encoder_type}" not supported!')

            
        if args.pretrained_embedding == 'bert':
            model_name = encoder_name_mapping[args.encoder_type]
            bert_embeddings = AutoModel.from_pretrained(model_name).embeddings

        if args.encoder_type == 'roberta':
            args.tokenizer_type = 'roberta'

        if args.tokenizer_type in ('bert', 'roberta'):
            tokenizier_name = encoder_name_mapping[args.tokenizer_type]

        tokenizer = AutoTokenizer.from_pretrained(tokenizier_name)
        self.cls_id = tokenizer.cls_token_id
        self.sep_id = tokenizer.sep_token_id
        self.ignore_tokens = set()

        # Ignore all tokens that doesn't contain an alphabetic character
        for token, token_id in tokenizer.vocab.items():
            if not bool(re.search('[a-zA-Z]', token)):
                self.ignore_tokens.add(token_id)
        ignore_tokens_list = list(self.ignore_tokens.union(tokenizer.all_special_ids))
        self.ignore_tokens = torch.tensor(ignore_tokens_list)
        self.ignore_tokens = self.ignore_tokens.unsqueeze(0).unsqueeze(1)

        self.matching_token_heads = []
        self.positional_heads = defaultdict(list)
        self.intersent_heads = []
        self.synthesized_heads = defaultdict(list)
        num_patterns = 4

        # Select the attention heads for masking/synthesizing
        # Arbitrarily set head indices (for vanilla Transformer)
        for j in range(self.args.heads_per_pattern):
            for layer_idx in range(model_config.num_hidden_layers):
                if args.match_token_attn != None:
                    self.matching_token_heads.append((layer_idx, num_patterns * j))
                    if args.match_token_attn == 'synthesize':
                        self.synthesized_heads[layer_idx].append(0)

                if args.inter_sent_attn != None:
                    self.intersent_heads.append((layer_idx, num_patterns * j + 1))
                    if args.inter_sent_attn == 'synthesize':
                        self.synthesized_heads[layer_idx].append(1)

                if args.positional_attn != None:
                    self.positional_heads[f'+1'].append((layer_idx, num_patterns * j + 2))
                    self.positional_heads[f'-1'].append((layer_idx, num_patterns * j + 3))
                    self.synthesized_heads[layer_idx].append(num_patterns * j + 2)
                    self.synthesized_heads[layer_idx].append(num_patterns * j + 3)


        # Add mask/fixed attention for the projected attention layers
        if self.use_pals and self.use_pal_patterns:
            for i in range(model_config.num_hidden_layers):
                self.matching_token_heads.append((i, 0))
                self.intersent_heads.append((i, 1))
                self.positional_heads['-1'].append((i, 2))
                self.positional_heads['+1'].append((i, 3))
                self.synthesized_heads[i].append(2)
                self.synthesized_heads[i].append(3)


        self.logger.info(f"Matching token heads: {self.matching_token_heads}")
        self.logger.info(f"Positional heads: {self.positional_heads}")
        self.logger.info(f"Inter-sentence heads: {self.intersent_heads}")

        # When maximum length for positional embeddings exceeds pretrained model (for NYT dataset)
        current_input_len = self.encoder.embeddings.position_embeddings.weight.shape[0]
        if args.max_input_len > current_input_len:
            positional_embeddings = nn.Embedding(args.max_input_len, self.encoder.config.hidden_size)
            weight_data = self.encoder.embeddings.position_embeddings.weight.data
            positional_embeddings.weight.data[:current_input_len] = weight_data
            positional_embeddings.weight.data[current_input_len:] = weight_data[-1][None, :].repeat(args.max_input_len - current_input_len, 1)
            self.encoder.embeddings.position_embeddings = positional_embeddings
            self.encoder.embeddings.register_buffer("position_ids", torch.arange(args.max_input_len).expand((1, -1)))

        self.max_input_len = self.encoder.config.max_position_embeddings

        # Extractive summarization layers (linear classifer or transformer)
        self.ext_layer = Classifier(self.encoder.config.hidden_size)


        # Initialize the parameters of summarization specific layers if a checkpoint is not loaded 
        if args.param_init != 0.0:
            for p in self.ext_layer.parameters():
                p.data.uniform_(-args.param_init, args.param_init)
        if args.param_init_glorot:
            for p in self.ext_layer.parameters():
                if p.dim() > 1:
                    nn.init.xavier_uniform_(p)

    def init_heads(self, heads_to_init: Dict[int, List[int]], zero_out=False):
        for layer, heads in heads_to_init.items():
            self.encoder.encoder.layer[layer].attention.init_heads(heads, zero_out=zero_out)


    def get_dynamic_mask(self, input_ids):
        cfg = self.encoder.config
        batch_size, input_len = input_ids.size()

        if self.use_pals and self.use_pal_patterns:
            mask_size = (batch_size, cfg.num_hidden_layers, cfg.num_attention_heads_pals, input_len, input_len)
        else:
            mask_size = (batch_size, cfg.num_hidden_layers, cfg.num_attention_heads, input_len, input_len)

        dynamic_attn_masks = torch.full(mask_size, True, dtype=torch.bool, device=self.device)

        if self.matching_token_heads != []:
            repeated_input_ids = input_ids.unsqueeze(1).repeat(1, input_len, 1)
            identical_mask = (repeated_input_ids.transpose(1, 2) == repeated_input_ids)
            del repeated_input_ids

            # Tokens with matching pairs does not attend to itself
            diag_mask = torch.eye(input_len).bool()
            identical_mask[:, diag_mask] = False

            # Special tokens will not attend to matching pairs for synthesized attention
            if self.args.match_token_attn == 'synthesize':
                self.ignore_tokens = self.ignore_tokens.to(self.device)
                ignore_mask = torch.any(input_ids.unsqueeze(2).repeat(1, 1, len(self.ignore_tokens)) == self.ignore_tokens.repeat(batch_size, input_len, 1), dim=2)
                identical_mask[ignore_mask] = False
                del ignore_mask

            # Tokens without matching pairs will attend to all tokens
            identical_mask[identical_mask.any(dim=-1).logical_not()] = True
            for (layer, head) in self.matching_token_heads:
                dynamic_attn_masks[:, layer, head] = identical_mask

            del identical_mask
            torch.cuda.empty_cache()


        cls_idx = [torch.nonzero(_input_ids == self.cls_id, as_tuple=False).squeeze(1) for _input_ids in input_ids]
        sep_idx = [torch.nonzero(_input_ids == self.sep_id, as_tuple=False).squeeze(1) for _input_ids in input_ids]
        if self.intersent_heads != []:
            inter_sent_mask = torch.full((batch_size, input_len, input_len), False, dtype=torch.bool, device=self.device)
            for batch_idx in range(batch_size):
                n_sent = len(cls_idx[batch_idx])
                for sent_idx in range(n_sent):
                    sent_start = cls_idx[batch_idx][sent_idx]
                    sent_end = sep_idx[batch_idx][sent_idx] + 1
                    inter_sent_mask[batch_idx][sent_start:sent_end, sent_start:sent_end] = True

            for layer, head in self.intersent_heads:
                dynamic_attn_masks[:, layer, head] = inter_sent_mask

        for key, indices in self.positional_heads.items():
            if indices != []:
                offset = int(key[1])
                positional_mask = torch.full((batch_size, input_len, input_len), False, dtype=torch.bool, device=self.device)
                
                for diagonal in range(1, offset + 1):
                    diagonal = -diagonal if key[0] == '-' else diagonal
                    offset_mask = torch.diag(torch.ones(input_len - abs(diagonal)), diagonal).bool().to(self.device)
                    positional_mask = torch.logical_or(positional_mask, offset_mask)
                
                for layer, head in indices:
                    dynamic_attn_masks[:, layer, head] = positional_mask


        # # Allow CLS to all tokens to attend to CLS and SEP tokens
        # for batch_idx in range(batch_size):
        #     cls_sep_idx = torch.cat((cls_idx[batch_idx], sep_idx[batch_idx]))
        #     dynamic_attn_masks[batch_idx, :, :, :, cls_sep_idx] = True

        return dynamic_attn_masks


    def forward(self, input_ids, seg_ids, input_mask, cls_idx, cls_mask, head_mask=None, output_attentions=False, **kwargs):
        
        dynamic_attn_mask = kwargs['dynamic_attn_mask']
        dynamic_attn_masks = self.get_dynamic_mask(input_ids) if dynamic_attn_mask else None
        if dynamic_attn_masks != None and dynamic_attn_masks.all().item():
            dynamic_attn_masks = None

        # Don't use segment IDs for Roberta encoder
        if self.args.encoder_type == 'roberta':
            seg_ids = None
        else:
            encoder_output = self.encoder(
                input_ids=input_ids,
                attention_mask=input_mask,
                token_type_ids=seg_ids,
                head_mask=head_mask,
                output_attentions=output_attentions,
                dynamic_attn_masks=dynamic_attn_masks,
                synthesized_heads=self.synthesized_heads,)


        hidden_states = encoder_output[0]
        batch_size = hidden_states.size(0)

        # Use the cls token representations as input for decoder
        span_vectors = hidden_states[torch.arange(hidden_states.size(0)).unsqueeze(1), cls_idx]
        span_vectors = span_vectors * cls_mask[:, :, None].float()

        # Compute the span scores based on the summarization layers
        span_scores = (self.ext_layer(span_vectors) * cls_mask.float()).squeeze(-1)


        if output_attentions:
            output = (span_scores, encoder_output[-1])
        else:
            output = span_scores

        return output