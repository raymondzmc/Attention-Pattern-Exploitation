# Human Guided Exploitation of Attention Patterns

Code base for our paper "Human Guided Exploitation of Interpretable Attention Patterns in Summarization and Topic Segmentation" presented at [EMNLP 2022](https://2022.emnlp.org/).

## Data Preprocessing

We adopt the code base from the original papers to preprocess the datasets.

Please refer to the https://github.com/nlpyang/PreSumm for summarization, and https://github.com/koomri/text-segmentation for topic segmentation.


## Running Summarization Models

For example, to train and evaluation the Transformer encoder with BERT embeddings, where the patterns are injected in all (12/12) heads on CNN/Daily Mail:

    python main.py -device 0 -mode test -result_dir ../results/cnndm_transformer_6-12_bert-embeddings_patterns_3/ \
    -config_file ./tinybert_model_config.json -dataset DiscoBERT_CNNDM_attn_map_new -batch_size 18 -top_n_ckpts 3 \
    -valid_metric rouge -data_dir /home/liraymo6/projects/def-carenini/liraymo6/Attention-Analysis/data -save_checkpoint_steps 2500  \
    -log_file test_top3.txt -top_n 3 -train_steps 100000 -match_token_attn mask -inter_sent_attn mask \
    -positional_attn synthesize -summarizer_type oursum -encoder_type transformer -pretrained_embedding bert -heads_per_pattern 3

    python main.py -device 0 -mode test -result_dir ./results/cnndm_transformer_6-12_bert-embeddings_patterns_3/ \
    -config_file ./tinybert_model_config.json -dataset DiscoBERT_CNNDM_attn_map_new -batch_size 18 -top_n_ckpts 3 \
    -valid_metric rouge -data_dir /home/liraymo6/projects/def-carenini/liraymo6/Attention-Analysis/data -save_checkpoint_steps 2500  \
    -log_file test_top3_triblck.txt -top_n 3 -train_steps 100000 -match_token_attn mask -inter_sent_attn mask \
    -positional_attn synthesize -summarizer_type oursum -encoder_type transformer -block_trigram -pretrained_embedding bert -heads_per_pattern 3 
