# Human Guided Exploitation of Attention Patterns

Code base for our paper "Human Guided Exploitation of Interpretable Attention Patterns in Summarization and Topic Segmentation" presented at [EMNLP 2022](https://2022.emnlp.org/).

## Data Preprocessing

We adopt the code base from the original papers to preprocess the datasets.

Please refer to the https://github.com/nlpyang/PreSumm for summarization, and https://github.com/koomri/text-segmentation for topic segmentation.


## Summarization

As an example, to train the Transformer encoder with BERT embeddings, where the patterns are injected in all (12/12) heads on CNN/Daily Mail (last row of Table 1):

    python run_summarization.py -device 0 -result_dir ./results/cnndm_transformer_6-12_bert-embeddings_patterns_3/ \
    -config_file ./tinybert_model_config.json -dataset DiscoBERT_CNNDM_attn_map_new -batch_size 18 -top_n_ckpts 3 \
    -valid_metric rouge -data_dir /home/liraymo6/projects/def-carenini/liraymo6/Attention-Analysis/data -save_checkpoint_steps 100  \
    -log_file log.txt -top_n 3 -train_steps 100000 -match_token_attn mask -inter_sent_attn mask -positional_attn synthesize \
    -summarizer_type oursum -encoder_type transformer -pretrained_embedding bert -heads_per_pattern 3

For evaluation you can run the same command with an additional argument ``-mode test``.
