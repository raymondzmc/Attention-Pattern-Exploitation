import os
import torch
from torch.utils.data import Dataset, DataLoader, IterableDataset
from torch.nn.utils.rnn import pad_sequence
import pdb
import numpy as np
from random import shuffle
from pathlib import Path

# class SummarizationDataset(Dataset):
#     """
#     Loads tokenized datasets from path, articles are stored in ".article" files,
#     stories are stored in ".story" files.
#     """

#     def __init__(self, args):
#         self.path = os.path.join(args.processed_dir, args.dataset)
#         self.file_list = os.listdir(self.path)

#         # self.data = []
#         # for file in self.file_list:
#         #     file_path = os.path.join(self.path, file)
#         #     self.data.extend(torch.load(file_path))

#     def __len__(self):
#         """ Returns the number of article-summary pairs """
#         return len(self.file_list)

#     def __getitem__(self, idx):

#         # See "preprocessing/data_builder.py" for saved format
#         return torch.load(os.path.join(self.path, self.file_list[idx]))


class BERTSummDataset(Dataset):
    """
    Loads tokenized datasets from path, articles are stored in ".article" files,
    stories are stored in ".story" files.
    """

    def __init__(self, args, dataset_type):
        self.path = os.path.join(args.data_dir, args.dataset, dataset_type)
        self.file_list = os.listdir(self.path)
        self.data = []
        self.dataset=args.dataset
        for file in self.file_list:
            file_path = os.path.join(self.path, file)
            self.data.extend(torch.load(file_path))

    def __len__(self):
        """ Returns the number of article-summary pairs """
        return len(self.data)

    def __getitem__(self, idx):
        return self.preprocessing(self.data[idx]), idx

    def preprocessing(self, data):
        out = {}
        out['id'] = data['doc_id'].split('.')[0]
        out['src'] = data['src']
        ###cnndm
        if self.dataset=='DiscoBERT_CNNDM_attn_map_new' :
            out['labels'] = data['labels'][0]
            out['segs'] = data['segs']
            out['clss'] = data['clss']+[len(data['segs'])]
        ###nyt
        # elif 'nyt' in self.dataset:
        #     out['labels'] = data['labels']
        #     # out['clss'] = np.where(np.array(out['src'])==101)[0].tolist()+[len(data['src'])]
        #     out['segs'] = data['segs']
        #     out['clss'] = data['clss']+[len(data['segs'])]
        else:
            out['labels'] = data['labels']
            out['segs'] = data['segs']
            out['clss'] = data['clss']+[len(data['segs'])]

        # d_span = data['d_span']

        # attnmap = data['attnmap'].type(torch.float)
        # if self.attnmap_type =='none':
        #     attnmap = data['attnmap'].type(torch.float)
        # # out['hi_attnmap'] = torch.load(self.attnmap_path+out['id']+'.out.attnmap')
        # elif self.attnmap_type =='dep_attnmask':
        #     attnmap = torch.tensor(torch.load(self.attnmap_path+out['id']+'.out.attnmap')['par_attention_mask']).type(torch.float)
        # elif self.attnmap_type =='attn_map_norm':
        #     attnmap = torch.tensor(torch.load(self.attnmap_path+out['id']+'.out.attnmap')).type(torch.float)

        # out['attnmap_word'] = self.get_attnmap_word(attnmap,d_span,out['src'])
        # attnmap_word = self.get_attnmap_word(attnmap,out['d_span'])
        # if use educlss
        # out['src'],out['clss'],out['segs'],out['d_span'] = self.edu_seg_input(data['src'],data['d_span'])

        out['sent_txt'] = [' '.join(s) for s in data['sent_txt']]
        out['tgt_txt'] = '\n'.join(data['tgt_list_str'])

        return out

    def get_attnmap_word(self,attnmap,d_span,src):
        # assert len(src)==d_span[-1][-1]

        num_edu = len(d_span)
        #batch * num_token * num_edu
        transfer_matrix = torch.zeros(len(src),num_edu)
        for j, edu in enumerate(d_span):
            transfer_matrix[edu[0]:edu[1],j]=1
        attnmap_word = torch.matmul(torch.matmul(transfer_matrix,attnmap),transfer_matrix.permute(1,0))
        return attnmap_word

class SummarizationDataset(IterableDataset):
    """
    Loads tokenized datasets from path, articles are stored in ".article" files,
    stories are stored in ".story" files.
    """

    def __init__(self, args, dataset_type):
        self.path = os.path.join(args.data_dir, args.dataset, dataset_type)
        # self.file_list = os.listdir(self.path)
        self.args = args
        inputs_dir = Path(self.path)
        self._input_files = [path for path in inputs_dir.glob("*.pt")]
        self._input_files = sorted(self._input_files)


        if dataset_type == 'train':
            self.is_test = False
            shuffle(self._input_files)
        else:
            self.is_test = True
            self.n_examples = sum([len(torch.load(x)) for x in self._input_files])

        self.dataset = args.dataset
        self.debug = args.debug

    def __len__(self):
        if self.is_test:
            return self.n_examples
        else:
            return None
        # for file in self.file_list:
        #     file_path = os.path.join(self.path, file)
        #     self.data.extend(torch.load(file_path))
    def _loaddata(self, idx):
        file = self._input_files[idx]
        self.cur_data = torch.load(file)
        if not self.is_test:
            shuffle(self.cur_data)
            if (idx==len(self._input_files)-1):
                shuffle(self._input_files)

    def __iter__(self):
        # for i in range(len(self._input_files)):
        if not self.is_test:
            i = 0
            while (True):
                self._loaddata(i)
                while len(self.cur_data) != 0:
                    data = self.cur_data.pop()
                    # if data['dependency_tree_embedding']:
                    # if data['tree_embedding']:
                    # if os.path.exists(self.attnmap_path+data['doc_id'].split('.')[0]+'.out.attnmap'):
                    if 'src' in data.keys():
                        try:
                            out = self.preprocessing(data)
                            yield out 
                        except Exception as e:
                            if self.debug:
                                print(f"Exception \"{e}\" occurred during preprocessing for ID {data['doc_id']} in file {self._input_files[i]}")
                            continue
                    else:
                        print(f"Attribute \"src\" unavailable for ID {data['doc_id']} in file {self._input_files[i]}")

                # After an epoch, shuffle the input_files
                if i == len(self._input_files) - 1:
                    shuffle(self._input_files)
                i = (i + 1) % (len(self._input_files))

        if self.is_test:
            for i in range(len(self._input_files)):
                self._loaddata(i)
                while len(self.cur_data) !=0:
                    data = self.cur_data.pop()
                    # if data['tree_embedding']:
                    # if data['dependency_tree_embedding']:
                    # if os.path.exists(self.attnmap_path+data['doc_id'].split('.')[0]+'.out.attnmap'):
                    if 'src' in data.keys():
                        try:
                            out = self.preprocessing(data)
                            yield out 
                        except Exception as e:
                            if self.debug:
                                print(f"Exception \"{e}\" occurred during preprocessing for ID {data['doc_id']} in file {self._input_files[i]}")
                            continue

    def preprocessing(self,data):
        out = {}
        out['id'] = data['doc_id'].split('.')[0]
        out['src'] = data['src']
        ###cnndm BERT
        if self.dataset=='DiscoBERT_CNNDM_attn_map_new':
            out['labels'] = data['labels'][0]
            out['segs'] = data['segs']
            out['clss'] = data['clss']+[len(data['segs'])]
        elif self.dataset=='DiscoRoBERTa_CNNDM_attn_map_new':
            out['labels'] = data['labels'][0]
            out['segs'] = data['segs']
            out['clss'] = data['clss']

        ###nyt
        elif 'nyt' in self.dataset:
            out['labels'] = data['labels']
            # out['clss'] = np.where(np.array(out['src'])==101)[0].tolist()+[len(data['src'])]
            out['segs'] = data['segs']
            out['clss'] = data['clss']
            # print(out['clss'])

            # # Use separator tokens to determine the length of each sentence
            # _segs = [-1] + [i for (i, t) in enumerate(out['src']) if t == 102]
            # segs = [_segs[i] - _segs[i - 1] for i in range(1, len(_segs))]

            # # Create binary masks as segment (sentence) embeddings 
            # segments_ids = []
            # for i, s in enumerate(segs):
            #     if (i % 2 == 0):
            #         segments_ids += s * [0]
            #     else:
            #         segments_ids += s * [1]
            # out['segs'] = segments_ids

        # d_span = data['d_span']


        # if data['attnmap'] == None:
        #     out['attnmap_word'] = torch.ones(len(out['src']), len(out['src']))/len(out['src'])
        #     if self.debug:
        #         print(f"Attribute \"attnmap\" is \"None\" for {data['doc_id']}")
        # else:
        #     attnmap = data['attnmap'].type(torch.float)
        #     if (attnmap.size(0) != len(d_span)):
        #         if self.debug:
        #             print(f"Mismatch between number of EDUs in 'attnmap' and 'transfer_matrix' for {data['doc_id']}")

        #         out['attnmap_word'] = torch.ones(len(out['src']), len(out['src']))/len(out['src'])
        #     else:
        #         out['attnmap_word'] = self.get_attnmap_word(attnmap,d_span,out['src'])
                # print(out['attnmap_word'])



        # if self.attnmap_type =='none':
        #     attnmap = data['attnmap'].type(torch.float)
        # # out['hi_attnmap'] = torch.load(self.attnmap_path+out['id']+'.out.attnmap')
        # elif self.attnmap_type =='dep_attnmask':
        #     attnmap = torch.tensor(torch.load(self.attnmap_path+out['id']+'.out.attnmap')['par_attention_mask']).type(torch.float)
        # elif self.attnmap_type =='attn_map_norm':
        #     attnmap = torch.tensor(torch.load(self.attnmap_path+out['id']+'.out.attnmap')).type(torch.float)

        # attnmap_word = self.get_attnmap_word(attnmap,out['d_span'])
        # if use educlss
        # out['src'],out['clss'],out['segs'],out['d_span'] = self.edu_seg_input(data['src'],data['d_span'])

        out['sent_txt'] = [' '.join(s) for s in data['sent_txt']]
        out['tgt_txt'] = '\n'.join(data['tgt_list_str'])

        return out

    def get_attnmap_word(self,attnmap,d_span,src):
        # assert len(src)==d_span[-1][-1]

        num_edu = len(d_span)
        #batch * num_token * num_edu
        transfer_matrix = torch.zeros(len(src),num_edu)
        cur_token_idx=0
        for j, edu in enumerate(d_span):
            transfer_matrix[edu[0]:edu[1],j]=1
        attnmap_word = torch.matmul(torch.matmul(transfer_matrix,attnmap),transfer_matrix.permute(1,0))
        special_tokens = torch.where(attnmap_word.sum(1)==0)[0]
        for i in special_tokens:
            attnmap_word[i] = torch.ones(attnmap_word.shape[1])
        return attnmap_word

# class Batch:
#     def __init__(self, data, encoder):
#         self.batch_size = len(data)

#         pad_token_id = 0 if encoder == 'bert' else 1

#         src = pad_sequence([torch.tensor(x['src']).long() for x, _ in data], batch_first=True, padding_value=pad_token_id)

#         try:
#             labels = pad_sequence([torch.tensor(x['labels']).float() for x, _ in data], batch_first=True, padding_value=pad_token_id)
#         except:
#             labels = pad_sequence([torch.tensor(x['src_sent_labels']).float() for x, _ in data], batch_first=True, padding_value=pad_token_id)
            
#         segs = pad_sequence([torch.tensor(x['segs']).long() for x, _ in data], batch_first=True, padding_value=pad_token_id)
#         mask = (src != pad_token_id).float()
#         clss = pad_sequence([torch.tensor(x['clss']) for x, _ in data], batch_first=True, padding_value=-1)
#         mask_cls = (clss != -1).float()
#         clss[clss == -1] = 0
#         src_txt = [x['src_txt'] for x, _ in data]
#         tgt_txt = [x['tgt_txt'] for x, _ in data]
#         indices = [idx for _, idx in data]

#         setattr(self, 'src', src)
#         setattr(self, 'labels', labels)
#         setattr(self, 'segs', segs)
#         setattr(self, 'mask', mask)
#         setattr(self, 'clss', clss)
#         setattr(self, 'mask_cls', mask_cls)
#         setattr(self, 'src_txt', src_txt)
#         setattr(self, 'tgt_txt', tgt_txt)
#         setattr(self, 'indices', indices)

#     def __len__(self):
#         return self.batch_size

#     # custom memory pinning method on custom type
#     def pin_memory(self):
#         self.src = self.src.pin_memory()
#         self.labels = self.labels.pin_memory()
#         self.segs = self.segs.pin_memory()
#         self.mask = self.mask.pin_memory()
#         self.clss = self.clss.pin_memory()
#         self.mask_cls = self.mask_cls.pin_memory()
#         return self


class Batch(object):

    def __init__(self, batch, max_length=512,encoder='bert'):
        """Create a Batch from a list of examples."""
        # batch=[x for x,_oh in data]
        # indices = [idx for _, idx in data]
        self.batch_size = len(batch)

        self.max_length = max_length
        batch = [self._cut(d, encoder) for d in batch]
        self.max_length = max([len(x['src']) for x in batch])
        # batch.sort(key=lambda x: len(x['d_span']), reverse=True)

        batch.sort(key=lambda x: len(x['labels']), reverse=True)
        clss_list = [x['clss'] for x in batch]
        
        pre_src = [x['src'] for x in batch]
        pre_labels = [x['labels'] for x in batch]
        pre_segs = [x['segs'] for x in batch]

        # pre_clss = [x['clss'][:-1] for x in batch]
        pre_clss = [x['clss'] for x in batch]


        # tree_embed = [x['tree_embed'] for x in batch]
        ids = [x['id'] for x in batch]
        # pre_attn_map_word = [x['attnmap_word'] for x in batch]

        pad_token_id = 0 if encoder == 'bert' else 1
        src = torch.tensor(self._pad(pre_src, pad_token_id))

        labels = torch.tensor(self._pad(pre_labels, 0)).type(torch.float)
        segs = torch.tensor(self._pad(pre_segs, 0))

        mask = (~(src == pad_token_id)).type(torch.float)
        sent_span = self._build_sent_span(clss_list,src.shape[1])
        
        #batch*edu_num
        sent_mask = (~(torch.sum(sent_span,dim=1) == 0)).type(torch.float)

        clss = torch.tensor(self._pad(pre_clss, -1))

        if clss.shape != labels.shape:
            pdb.set_trace()
        mask_cls = ~(clss == -1)
        # print(mask_cls)
        clss[clss == -1] = 0

        # attn_map = torch.zeros(src.shape[0],src.shape[1],src.shape[1])
        # for i, attnmap_single in enumerate(pre_attn_map_word):
        #     attn_map[i,:attnmap_single.shape[0],:attnmap_single.shape[1]]=attnmap_single
        #     attn_map[i,attnmap_single.shape[0]:]=torch.ones(src.shape[1]-attnmap_single.shape[0],src.shape[1])

        setattr(self, 'src', src)
        setattr(self, 'segs', segs)
        setattr(self, 'mask', mask)
        setattr(self, 'sent_span', sent_span)
        setattr(self, 'clss', clss)
        setattr(self, 'mask_cls', mask_cls)
        setattr(self, 'labels', labels)
        setattr(self, 'sent_mask', sent_mask)
        setattr(self, 'ids', ids)
        src_str = [x['sent_txt'] for x in batch]
        setattr(self, 'src_txt', src_str)
        tgt_str = [x['tgt_txt'] for x in batch]
        setattr(self, 'tgt_txt', tgt_str)
        setattr(self, 'indices', ids)

        # setattr(self, 'attn_map', attn_map)

    def __len__(self):
        return self.batch_size

    def _pad(self, data, pad_id,width=-1):
        if (width == -1):
            width = max(len(d) for d in data)
        rtn_data = [d + [pad_id] * (width - len(d)) for d in data]
        return rtn_data

    def _cut(self, data, encoder='bert'):
        if len(data['src']) > self.max_length:

            # Find the last seperator token as the end of the document
            sep_token_id = 102 if encoder == 'bert' else 2

            sep_token_indices = np.where(np.array(data['src']) == sep_token_id)[0]
            valid_sep_token_indices = sep_token_indices[sep_token_indices < self.max_length]
            end_idx = valid_sep_token_indices[-1]

            # Number of tokens and spans (extractive candidates) after truncating
            n_tokens = end_idx + 1
            n_spans = len(valid_sep_token_indices)

            data['src'] = data['src'][:n_tokens]
            data['segs'] = data['segs'][:n_tokens]
            data['clss'] = data['clss'][:n_spans]
            data['labels'] = data['labels'][:n_spans]
            # data['attnmap_word'] = data['attnmap_word'][:n_tokens, :n_tokens].type(torch.float)

        elif len(data['labels']) < len(data['clss']):
            n_spans = len(data['labels'])
            data['clss'] = data['clss'][:n_spans]

        return data

    def _build_sent_span(self,clss_list,length):
        max_sent = max([len(clss) for clss in clss_list])-1
        #batch * num_token * num_sentence
        sent_span = torch.zeros(self.batch_size,length,max_sent)
        for i,clss in enumerate(clss_list):
            for j, sent in enumerate(clss[:-1]):
                sent_span[i,clss[j]+1:clss[j+1]-1,j]=1
        return sent_span




def collate_fn(batch, max_length, encoder):
    return Batch(batch, max_length,  encoder)



def get_dataloader(args, dataset_type):

    # dataset = BERTSummDataset(args, dataset_type)
    dataset = SummarizationDataset(args, dataset_type)

    models = ['transformer', 'tiny-bert', 'distillbert', 'bert-6L']
    if hasattr(args, 'encoder_type'):
        if args.encoder_type in models:
            encoder_type = args.tokenizer_type
        else:
            encoder_type = args.encoder_type
    else:
        encoder_type = 'bert'


    # TODO: add collate_fn to batch dataloader so this works
    dataloader = DataLoader(dataset,
                            batch_size=args.batch_size if dataset_type == 'train' else args.eval_batch_size,
                            # shuffle=(dataset_type=='train'),
                            num_workers=args.num_workers,
                            collate_fn=lambda x: collate_fn(x, args.max_input_len, encoder_type),
                            pin_memory=True)

    return dataloader

