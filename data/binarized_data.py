# coding=utf-8
# Copyright 2019-present, the HuggingFace Inc. team.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Preprocessing script before distillation.
"""
import argparse
import logging
import pickle
import random
import time
import math
import gc
import os

import numpy as np
import multiprocessing
import multiprocess as mp
import itertools
from tqdm import tqdm
import pdb

# cPickle for handling large file
import _pickle as pickel
import json

from transformers import BertTokenizer, GPT2Tokenizer, RobertaTokenizer


logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(name)s - %(message)s", datefmt="%m/%d/%Y %H:%M:%S", level=logging.INFO
)
logger = logging.getLogger(__name__)


def chunk_data(args, data_dir, save_dir):
    assert os.path.isdir(data_dir)
    os.makedirs(save_dir, exist_ok=True)

    data = []
    for file in os.listdir(data_dir):
        if file.startswith('dump'):
            file_path = os.path.join(data_dir, file)
            with open(file_path, 'rb') as file:
                data.extend(pickle.load(file))
            print(f"Loaded data file \"{file_path}\"")

    old_len = len(data)
    data = [ex for ex in data if len(ex) > 11]
    data = remove_unknown_and_long_sequences(args, data)

    new_len = len(data)
    print(f"Kept {new_len}/{old_len} examples.")

    chunk_size = 100000
    n_chunks = math.ceil(len(data) / chunk_size)

    for idx in tqdm(range(n_chunks)):
        start_idx, end_idx = idx * chunk_size, (idx + 1) * chunk_size
        with open(os.path.join(save_dir, f'dump.bert-base-uncased.{idx + 1}.pickle'), 'wb') as outfile:
            pickle.dump(data[start_idx: end_idx], outfile)

    print(f"Finished saving chunked data!")
    return data

def remove_unknown_and_long_sequences(args, data):
    """
    Remove sequences with a (too) high level of unknown tokens.
    """
    lengths = np.array([len(t) for t in data])
    tokenizer = BertTokenizer.from_pretrained(args.tokenizer_name)
    max_model_input_size = tokenizer.max_model_input_sizes[args.tokenizer_name]
    indices = lengths > max_model_input_size
    print(f"Splitting {sum(indices)} too long sequences.")

    special_tok_ids = {}
    for tok_name, tok_symbol in tokenizer.special_tokens_map.items():
        idx = tokenizer.all_special_tokens.index(tok_symbol)
        special_tok_ids[tok_name] = tokenizer.all_special_ids[idx]

    if "unk_token" not in special_tok_ids:
        return
    else:
        unk_token_id = special_tok_ids["unk_token"]

    init_size = len(data)
    unk_occs = np.array([np.count_nonzero(a == unk_token_id) for a in data])
    indices = (unk_occs / lengths) < 0.5
    data = [t for i, t in enumerate(data) if indices[i]]
    new_size = len(data)
    print(f"Remove {init_size - new_size} sequences with a high level of unknown tokens (50%).")
    return data

def main():
    parser = argparse.ArgumentParser(
        description="Preprocess the data to avoid re-doing it several times by (tokenization + token_to_ids)."
    )
    parser.add_argument("--file_path", type=str, default="data/dump.txt", help="The path to the data.")
    parser.add_argument("--tokenizer_type", type=str, default="bert", choices=["bert", "roberta", "gpt2"])
    parser.add_argument("--tokenizer_name", type=str, default="bert-base-uncased", help="The tokenizer to use.")
    parser.add_argument("--dump_file", type=str, default="data/dump", help="The dump file prefix.")
    parser.add_argument("--num_processes", type=int, default=(mp.cpu_count() - 1), help="The number of processes to spawn.")
    args = parser.parse_args()

    

    chunk_data(args, 'bookcorpus_wiki', 'bookcorpus_wiki_chunked')




    # logger.info(f"Loading text from {args.file_path}")
    # with open(args.file_path, "r", encoding="utf8") as fp:
    #     data = fp.readlines()

    # # # Divide data into two halfs
    # cutoff_index = len(data) // 3
    # data = data[cutoff_index : cutoff_index * 2]

    # n_examples = len(data)
    # logger.info("Start encoding")
    # logger.info(f"{len(data)} examples to process.")

    # num_per_process = math.ceil(n_examples / args.num_processes)
    # split_data = [(i, data[i * num_per_process: (i + 1) * num_per_process]) for i in range(args.num_processes)]

    # global process_data
    # def process_data(chunk):
    #     index, data = chunk
    #     chunk_len = len(data)
    #     logger.info(f"Loading Tokenizer ({args.tokenizer_name})")
    #     if args.tokenizer_type == "bert":
    #         tokenizer = BertTokenizer.from_pretrained(args.tokenizer_name)
    #         bos = tokenizer.special_tokens_map["cls_token"]  # `[CLS]`
    #         sep = tokenizer.special_tokens_map["sep_token"]  # `[SEP]`
    #     elif args.tokenizer_type == "roberta":
    #         tokenizer = RobertaTokenizer.from_pretrained(args.tokenizer_name)
    #         bos = tokenizer.special_tokens_map["cls_token"]  # `<s>`
    #         sep = tokenizer.special_tokens_map["sep_token"]  # `</s>`
    #     elif args.tokenizer_type == "gpt2":
    #         tokenizer = GPT2Tokenizer.from_pretrained(args.tokenizer_name)
    #         bos = tokenizer.special_tokens_map["bos_token"]  # `<|endoftext|>`
    #         sep = tokenizer.special_tokens_map["eos_token"]  # `<|endoftext|>`
    #     vocab_size = tokenizer.vocab_size
    #     interval = 100000
    #     iter = 0
    #     start = time.time()
    #     rslt = []
    #     current = mp.current_process()

    #     for text in data:
    #         text = f"{bos} {text.strip()} {sep}"
    #         token_ids = tokenizer.encode(text, add_special_tokens=False)

    #         # For examples less than max length
    #         if len(token_ids) <= tokenizer.model_max_length:

    #             # Append tokens to results
    #             if vocab_size < (1 << 16):
    #                 rslt.append(np.uint16(token_ids))
    #             else:
    #                 rslt.append(np.int32(token_ids))

    #         iter += 1
    #         if iter % interval == 0:
    #             end = time.time()
    #             print(f"[{index + 1}/{args.num_processes}] {iter}/{len(data)} examples processed. - {(end-start):.2f}s/{interval}expl")
    #             start = time.time()

    #     # Remove references to raw data
    #     del chunk, data
    #     gc.collect()

    #     logger.info(f"[{index + 1}/{args.num_processes}] Finished binarization")
    #     logger.info(f"[{index + 1}/{args.num_processes}] {chunk_len} examples processed.")
    #     dp_file = f"{args.dump_file}.{args.tokenizer_name}_{index + 1}.pickle"
    #     with open(dp_file, "wb") as handle:
    #         _pickle.dump(rslt, handle)
    #     logger.info(f"Dumped to {dp_file}")
    #     # return rslt

            

    # with mp.get_context("spawn").Pool(processes=args.num_processes) as pool:
    #     pool.map(process_data, split_data)

    # logger.info("Finished binarization")
    # logger.info(f"{n_examples} examples processed.")

    # # dp_file = f"{args.dump_file}.{args.tokenizer_name}.pickle"
    # # logger.info(f"Dump to {dp_file}")
    # # with open(dp_file, "wb") as handle:
    # #     _pickle.dump(results, handle)    
    # # results = list(itertools.chain.from_iterable(results))
    # # random.shuffle(results)

    # results = []
    # for index in range(args.num_processes):
    #     chunked_dp_file = f"{args.dump_file}.{args.tokenizer_name}_{index + 1}.pickle"
    #     with open(chunked_dp_file, 'rb') as handle:
    #         chunked_rslt = _pickle.load(handle)
    #         results.extend(chunked_rslt)
    # # pdb.set_trace()

    # dp_file = f"{args.dump_file}.{args.tokenizer_name}.pickle"    
    # with open(dp_file, 'wb') as handle:
    #     _pickle.dump(results, handle)


if __name__ == "__main__":
    main()
