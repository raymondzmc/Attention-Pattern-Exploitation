import re
import os
import json
import math
import itertools
import torch

from torch.utils.data import Dataset
from transformers import AutoTokenizer
from nltk.tokenize import RegexpTokenizer
from pathlib2 import Path
from tqdm import tqdm
from os.path import join as pjoin

import utils
import pdb


# logger = utils.setup_logger(__name__, 'train.log')

section_delimiter = "========"

def get_seperator_foramt(levels = None):
    segment_seperator = "========"
    level_format = '\d' if levels == None else '['+ str(levels[0]) + '-' + str(levels[1]) + ']'
    seperator_fromat = segment_seperator + ',' + level_format + ",.*?\."
    return seperator_fromat


def get_list_token():
    return "***LIST***"

def get_formula_token():
    return "***formula***"

def get_codesnipet_token():
    return "***codice***"

def get_special_tokens():
    special_tokens = []
    special_tokens.append(get_list_token())
    special_tokens.append(get_formula_token())
    special_tokens.append(get_codesnipet_token())
    return special_tokens

def get_words_tokenizer():
    global words_tokenizer

    if words_tokenizer:
        return words_tokenizer

    words_tokenizer = RegexpTokenizer(r'\w+')
    return words_tokenizer


def extract_sentence_words(sentence, remove_missing_emb_words = False,remove_special_tokens = False):
    if (remove_special_tokens):
        for token in get_special_tokens():
            # Can't do on sentence words because tokenizer delete '***' of tokens.
            sentence = sentence.replace(token, "")
    tokenizer = get_words_tokenizer()
    sentence_words = tokenizer.tokenize(sentence)
    if remove_missing_emb_words:
        sentence_words = [w for w in sentence_words if w not in missing_stop_words]

    return sentence_words

def word_model(word, model):
    if model is None:
        return np.random.randn(1, 300)
    else:
        if word in model:
            return model[word].reshape(1, 300)
        else:
            #print ('Word missing w2v: ' + word)
            return model['UNK'].reshape(1, 300)



def get_files(path):
    all_objects = Path(path).glob('**/*')
    files = [str(p) for p in all_objects if p.is_file()]
    return files


def get_cache_path(wiki_folder):
    cache_file_path = Path(wiki_folder) / 'paths_cache'
    return cache_file_path


def cache_wiki_filenames(wiki_folder):
    files = Path(wiki_folder).glob('*/*/*/*')
    files = [f for f in files if len(str(f).split('/')[-1].split('_')) == 1]
    cache_file_path = get_cache_path(wiki_folder)
    with cache_file_path.open('w+') as f:
        for file in files:
            f.write(str(file) + '\n')


def clean_section(section):
    cleaned_section = section.strip('\n')
    return cleaned_section


def get_scections_from_text(txt, high_granularity=True):
    sections_to_keep_pattern = get_seperator_foramt() if high_granularity else get_seperator_foramt(
        (1, 2))
    if not high_granularity:

        # if low granularity required we should flatten segments within segemnt level 2
        pattern_to_ommit = get_seperator_foramt((3, 999))
        txt = re.sub(pattern_to_ommit, "", txt)

        #delete empty lines after re.sub()
        sentences = [s for s in txt.strip().split("\n") if len(s) > 0 and s != "\n"]
        txt = '\n'.join(sentences).strip('\n')


    all_sections = re.split(sections_to_keep_pattern, txt)
    non_empty_sections = [s for s in all_sections if len(s) > 0]

    return non_empty_sections


def get_sections(path, high_granularity=True):
    file = open(str(path), "r")
    raw_content = file.read()
    file.close()

    clean_txt = raw_content.strip()

    sections = [clean_section(s) for s in get_scections_from_text(clean_txt, high_granularity)]

    return sections


def read_wiki_file(path, word2vec, remove_preface_segment=True, ignore_list=False, remove_special_tokens=False,
                   return_as_sentences=True, high_granularity=True, only_letters = False):
    data = []
    targets = []

    # Get different sections based on patterns specified by data
    all_sections = get_sections(path, high_granularity)

    # Remove preface and empty sections
    required_sections = all_sections[1:] if remove_preface_segment and len(all_sections) > 0 else all_sections
    required_non_empty_sections = [section for section in required_sections if len(section) > 0 and section != "\n"]


    for section in required_non_empty_sections:
        sentences = section.split('\n')
        if sentences:
            for sentence in sentences:
                is_list_sentence = get_list_token() + "." == sentence
                if ignore_list and is_list_sentence:
                    continue
                if not return_as_sentences:
                    sentence_words = extract_sentence_words(sentence, remove_special_tokens=remove_special_tokens)
                    if 1 <= len(sentence_words):
                        data.append([word_model(word, word2vec) for word in sentence_words])
                    else:
                        #raise ValueError('Sentence in wikipedia file is empty')
                        logger.info('Sentence in wikipedia file is empty')
                else:  # for the annotation. keep sentence as is.
                    if (only_letters):
                        sentence = re.sub('[^a-zA-Z0-9 ]+', '', sentence)
                        data.append(sentence)
                    else:
                        data.append(sentence)
            if data:
                # Index of the last sentence in the current section
                targets.append(len(data) - 1)
    return data, targets, path


class WikipediaDataSet(Dataset):
    def __init__(self, root, word2vec, train=True, manifesto=False, folder=False, high_granularity=False):

        if (manifesto):
            self.textfiles = list(Path(root).glob('*'))
        else:
            if (folder):
                self.textfiles = get_files(root)
            else:
                root_path = Path(root)
                cache_path = get_cache_path(root_path)
                if not cache_path.exists():
                    cache_wiki_filenames(root_path)

                # Stores the path to all files in the dataset
                self.textfiles = cache_path.read_text().splitlines()

        if len(self.textfiles) == 0:
            raise RuntimeError('Found 0 images in subfolders of: {}'.format(root))
        self.train = train
        self.root = root
        self.word2vec = word2vec
        self.high_granularity = high_granularity

    def __getitem__(self, index):
        path = self.textfiles[index]

        return read_wiki_file(Path(path), self.word2vec, ignore_list=True, remove_special_tokens=True,
                              high_granularity=self.high_granularity)

    def __len__(self):
        return len(self.textfiles)



class CrossSegWikiSectionDataset(Dataset):
    """
    WikiSection dataset formatted for the cross-segment attention model
    """
    def __init__(self, args, split_name, domain='city'):
        assert domain in ['city', 'disease']

        self.high_granularity = args.high_granularity
        self.context_len = args.context_len
        self.pad_context = args.pad_context

        if args.encoder == 'transformer':
            # Use BERT tokenizer for vanilla Transformers
            tokenizer_name = 'bert-base-uncased'
            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
        else:
            tokenizer_name = args.encoder
            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)

        # During analysis, use sentence boundaries to evaluate intra-sent attention
        self.return_sent_boundaries = True if args.mode == 'analysis' else False


        encoder_name = tokenizer_name.split('/')[-1]
        cache_name = f'wikisection_en_{domain}_{split_name}_{encoder_name}_{args.context_len}.pth'
        cached_processed_file = pjoin(args.data_dir, cache_name)

        if not os.path.isfile(cached_processed_file):
            json_file = pjoin(args.data_dir, f'wikisection_en_{domain}_{split_name}.json')
            assert os.path.isfile(json_file)
            with open(json_file, 'r') as f:
                json_data = json.load(f)

            preprocessed = self.preprocess_for_encoder(json_data)
            torch.save(preprocessed, cached_processed_file)
        else:
            preprocessed = torch.load(cached_processed_file)


        self.data, self.index2data = preprocessed['data'], preprocessed['index2data']
        self.num_boundaries = len(self.index2data.keys())
    
    def preprocess_for_encoder(self, data):

        # Number of sentence boundaries
        num_boundaries = 0

        # Number of valid examples (more than one sentences), also used as file/example index
        num_examples = 0

        # For storing processed model input
        processed_data = []
        index2data = {}

        for idx, example in tqdm(enumerate(data), total=len(data)):
            text, annotations = example['text'], example['annotations']

            sections = []
            for sec in annotations:
                begin = sec['begin']
                end = begin + sec['length'] 
                sections.append(text[begin:end].strip().split('\n'))

            seg_idx = list(map(lambda x: x - 1, itertools.accumulate([len(sec) for sec in sections])))[:-1]
            sents = [sent for sec in sections for sent in sec]

            # sents_from_text = text.strip().split('\n')
            # assert sents_from_text == sents

            if len(sents) < 2:
                continue

            input_ids = self.tokenizer(sents, add_special_tokens=False, return_token_type_ids=False, return_attention_mask=False)['input_ids']
            sent_boundaries = list(itertools.accumulate([len(sent) for sent in input_ids]))[:-1]
            input_ids = [token_id for sent in input_ids for token_id in sent]
            targets = [1 if idx in seg_idx else 0 for idx in range(len(sent_boundaries))]

            for boundary_idx in range(len(sent_boundaries)):
                dataset_idx = num_boundaries + boundary_idx
                index2data[dataset_idx] = (num_examples, boundary_idx)


            num_boundaries += len(sent_boundaries)
            num_examples += 1


            processed_data.append({'input_ids': input_ids, 'sent_boundaries': sent_boundaries, 'targets': targets})

        return {'data': processed_data, 'index2data': index2data}


    def __len__(self):
        return self.num_boundaries

    def __getitem__(self, index):
        data_idx, boundary_idx = self.index2data[index]
        data = self.data[data_idx]
        token_idx = data['sent_boundaries'][boundary_idx]
        
        start, end = max(0, token_idx - self.context_len), token_idx + self.context_len
        left_context = data['input_ids'][start:token_idx]
        right_context = data['input_ids'][token_idx:end]

        # For returning sentence boundaries (intervals)
        sent_boundaries = [0] + data['sent_boundaries'] + [len(data['input_ids'])]

        # Pad to the left or right of context
        if self.pad_context:
            left_pad, right_pad = self.context_len - len(left_context), self.context_len - len(right_context)
            left_context = [self.tokenizer.pad_token_id for _ in range(left_pad)] + left_context
            right_context = right_context + [self.tokenizer.pad_token_id for _ in range(right_pad)]
            sent_boundaries = list(map(lambda idx: idx + left_pad, sent_boundaries))
            



        input_ids = [self.tokenizer.cls_token_id] + left_context + [self.tokenizer.sep_token_id] + right_context
        token_type_ids = [0 for _ in range(len(left_context) + 2)] + [1 for _ in range(len(right_context))]
        attention_mask = [0 if x == self.tokenizer.pad_token_id else 1 for x in input_ids]

        if self.return_sent_boundaries:
            # Compensate for [CLS] and [SEP] tokens
            sent_start = [(idx + 1 if i <= boundary_idx else idx + 2) for i, idx in enumerate(sent_boundaries[:-1])]
            sent_end = [(idx + 1 if i <= (boundary_idx + 1) else idx + 2) for i, idx in enumerate(sent_boundaries[1:])]
            sent_intervals = list(zip(sent_start, sent_end))
            return (input_ids, token_type_ids, attention_mask, data['targets'][boundary_idx], sent_intervals)
        else:
            return (input_ids, token_type_ids, attention_mask, data['targets'][boundary_idx])




class CrossSegWiki727KDataset(Dataset):
    """
    Wikipedia topic segmentation dataset formatted for the cross-segment attention model
    """
    def __init__(self, args, split_name):
        self.data_dir = pjoin(args.data_dir, split_name)
        self.data_root_dir = args.data_dir
        self.high_granularity = args.high_granularity
        self.context_len = args.context_len
        self.pad_context = args.pad_context

        tokenizer_name = 'bert-base-uncased' if args.encoder == 'transformer' else args.encoder
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)

        # Transformer models use BERT tokenizer
        if args.encoder == 'transformer':
            self.file_suffix = 'bert-base-uncased'
        else:
            self.file_suffix = args.encoder.split('/')[-1]


        processed_data_path = pjoin(self.data_root_dir, f'wiki727_{split_name}_{self.file_suffix}_{self.context_len}.pt')
        if os.path.isfile(processed_data_path):
            processed_data = torch.load(processed_data_path)
            self.data = processed_data['data']
            self.index2data = processed_data['index2data']
            self.num_boundaries = processed_data['num_boundaries']
            self.num_documents = processed_data['num_documents']
            
        else:

            json_file_path = pjoin(self.data_root_dir, f'wiki727_{split_name}.json')

            if os.path.isfile(json_file_path):
                # Load the json file containing pre-processed data files
                with open(json_file_path, 'r') as f:
                    json_data = json.load(f)
            else:
                assert split_name in os.listdir(args.data_dir)

                # Load cache file containing absolute path to each data file
                cached_file_path = pjoin(self.data_dir, 'paths_cache')
                if not os.path.isfile(cached_file_path):
                    cache_wiki_filenames(self.data_dir)
                    print(f"Created cache file containing paths to all data files at: \"{cached_file_path}\"")

                # Stores the path to all files in the dataset
                data_files = Path(cached_file_path).read_text().splitlines()

                # Save pre-processed data files to a single json file
                json_data = self.save_json_file(data_files, json_file_path)

            # Only use 10% of data for development set
            if split_name == 'dev':
                json_data = json_data[:len(json_data) // 10]

            print(f"Preprocessing Wiki-727K Dataset for \"{args.encoder}\"")
            n_files = 5 if split_name == 'train' else 1
            processed_data = self.process_for_encoder(json_data)
            torch.save(processed_data, processed_data_path)

    def save_json_file(self, data_files, json_file_path):
        
        json_data = []
        for idx, file_path in enumerate(tqdm(data_files)):
            (data, seg_idx, path) = read_wiki_file(Path(file_path), None, ignore_list=True, remove_special_tokens=True, high_granularity=self.high_granularity)
            json_data.append({
                'idx': idx,
                'data': data,
                'seg_idx': seg_idx,
            })

        with open(json_file_path, 'w+') as json_file:
            json.dump(json_data, json_file, indent=4)

        return json_data

    def process_for_encoder(self, json_data):
        """
        Save all preprocessed data files for the respective encoder by using the encoder name as file name suffix
        (e.g) name -> name_tiny-bert
        """


        # Number of documents (greater than 2 sentences)
        self.num_documents = 0

        # Number of sentence boundaries
        self.num_boundaries = 0
        
        # Map index to the corresponding sentence boundary in the data file
        self.index2data = {}

        self.data = []
        for i, document in enumerate(tqdm(json_data)):
            file_idx, data, seg_idx = document['idx'], document['data'], document['seg_idx']

            if len(data) < 2:
                continue

            input_ids = self.tokenizer(
                [[sent] for sent in data],
                is_split_into_words=True,
                add_special_tokens=False,
                return_token_type_ids=False,
                return_attention_mask=False
            )['input_ids']

            sent_boundaries = list(itertools.accumulate([len(sent) for sent in input_ids]))[:-1]
            input_ids = [token_id for sent in input_ids for token_id in sent]
            targets = [0 for _ in range(len(sent_boundaries))]
            for idx in seg_idx[:-1]:
                targets[idx] = 1

            # targets = [0 for idx in range(len(sent_boundaries)) if idx in seg_idx else 0]

            # Use string representation to save space
            self.data.append(json.dumps({
                'idx': file_idx,
                'input_ids': input_ids,
                'sent_boundaries': sent_boundaries,
                'targets': targets
            }))

            for boundary_idx in range(len(data) - 1):
                dataset_idx = self.num_boundaries + boundary_idx
                self.index2data[dataset_idx] = (self.num_documents, boundary_idx)
            
            self.num_documents += 1
            self.num_boundaries += len(data) - 1



        processed_data = {
            'data': self.data,
            'index2data': self.index2data,
            'num_boundaries': self.num_boundaries,
            'num_documents': self.num_documents,
        }

        return processed_data

    def __len__(self):
        return self.num_boundaries

    def __getitem__(self, index):
        file_idx, boundary_idx = self.index2data[index]
        data = json.loads(self.data[file_idx])
        try:
            token_idx = data['sent_boundaries'][boundary_idx]
        except:
            pdb.set_trace()
        left_context = data['input_ids'][max(0, token_idx - self.context_len):token_idx]
        right_context = data['input_ids'][token_idx:token_idx + self.context_len]

        # Pad to the left or right of context
        if self.pad_context:
            left_context = [self.tokenizer.pad_token_id for _ in range(self.context_len - len(left_context))] + left_context
            right_context = right_context + [self.tokenizer.pad_token_id for _ in range(self.context_len - len(right_context))]

        input_ids = [self.tokenizer.cls_token_id] + left_context + [self.tokenizer.sep_token_id] + right_context
        token_type_ids = [0 for _ in range(len(left_context) + 2)] + [1 for _ in range(len(right_context))]
        attention_mask = [0 if x == 0 else 1 for x in input_ids]
        return (input_ids, token_type_ids, attention_mask, data['targets'][boundary_idx]) 



    


